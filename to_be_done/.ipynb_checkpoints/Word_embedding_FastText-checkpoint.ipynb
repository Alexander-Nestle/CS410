{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras) (5.1.2)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 12.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras) (1.14.3)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.3.1 keras-applications-1.0.8 keras-preprocessing-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define Classes\n",
    "class DocumentItem:\n",
    "    \"\"\"This class defines the structure of one touristic site\"\"\"\n",
    "    def __init__(self, key, name, address, review_text):\n",
    "        self.key = key\n",
    "        self.name = name\n",
    "        self.address = address\n",
    "        self.review = review_text\n",
    "        self.doc_len = 1\n",
    "\n",
    "    \"\"\"Set document length, note this length are counted in words and have stop words removed\"\"\"\n",
    "    def set_doclen(self, doc_len):\n",
    "        self.doc_len = doc_len\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"This class defines the structure that serves for all documents\"\"\"\n",
    "    def __init__(self):\n",
    "        self.doc_num = 0\n",
    "        self.avg_dl = 0\n",
    "\n",
    "    \"\"\"Add one document length here, and document number and average length will be changed accordingly\"\"\"\n",
    "    def add_doc(self, doc_len):\n",
    "        self.doc_num += 1\n",
    "        if self.doc_num > 0:\n",
    "            self.avg_dl = (self.avg_dl * (self.doc_num - 1) + doc_len)/self.doc_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parse the review documents into documentItems\"\"\"\n",
    "class JsonParser:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.doc_list = []\n",
    "        self.doc_idx = 0\n",
    "\n",
    "    def parse(self):\n",
    "        with open(self.filename, 'rb') as f:\n",
    "            self.doc_list = json.load(f)\n",
    "\n",
    "    def has_more_item(self):\n",
    "        return self.doc_idx < len(self.doc_list)\n",
    "\n",
    "    def get_next_item(self):\n",
    "        if len(self.doc_list) == 0:\n",
    "            print(\"Error: need to parse before getitem\")\n",
    "            return\n",
    "        if self.doc_idx >= len(self.doc_list):\n",
    "            print(\"Error: index has exceeded maximum item numbers\")\n",
    "            return\n",
    "        item_raw = self.doc_list[self.doc_idx]\n",
    "        item_key = item_raw['place_id']\n",
    "        item_name = item_raw['name']\n",
    "        item_address = item_raw['formatted_address']\n",
    "        item_text = \"\"\n",
    "        for item_rev in item_raw['reviews']:\n",
    "            if \"language\" in item_rev.keys() and item_rev[\"language\"].lower() != \"en\":\n",
    "                continue\n",
    "            item_text += item_rev[\"text\"]\n",
    "            item_text += \" \"\n",
    "            # I guess we could add some filter here for the \"time\" attribute\n",
    "\n",
    "        self.doc_idx += 1\n",
    "        return DocumentItem(item_key, item_name, item_address, item_text)\n",
    "\n",
    "''' Test Step 2, create json parser on top of one json file'''\n",
    "json_parser = JsonParser(\"PlacesResults.json\")\n",
    "json_parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
